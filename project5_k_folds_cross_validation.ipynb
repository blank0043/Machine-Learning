{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blank0043/Machine-Learning/blob/main/k_folds_cross_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pceu8bBsZBDk"
      },
      "source": [
        "<a id=\"k-folds-cross-validation\"></a>\n",
        "## K-Folds Cross-Validation\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "2bmrSqrGZBDl"
      },
      "source": [
        "Train/test split provides us with helpful tool, but it's a shame that we are tossing out a large chunk of our data for testing purposes.\n",
        "\n",
        "**How can we use the maximum amount of our data points while still ensuring model integrity?**\n",
        "\n",
        "1. Split our data into a number of different pieces (folds).\n",
        "2. Train using `k-1` folds for training and a different fold for testing.\n",
        "3. Average our model against EACH of those iterations.\n",
        "4. Choose our model and TEST it against the final fold.\n",
        "5. Average all test accuracies to get the estimated out-of-sample accuracy.\n",
        "\n",
        "Although this may sound complicated, we are just training the model on k separate train-test-splits, then taking the average of the resulting test accuracies!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSsWCezEZBDl"
      },
      "source": [
        "<a id=\"leave-one-out-cross-validation\"></a>\n",
        "### Leave-One-Out Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "aQS1jl2ZZBDl"
      },
      "source": [
        "A special case of k-fold cross-validation is leave-one-out cross-validation. Rather than taking 5–10 folds, we take a fold of size `n-1` and leave one observation to test. \n",
        "\n",
        "Typically, 5–10 fold cross-validaiton is recommended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4nhWNvuZBDl"
      },
      "source": [
        "<a id=\"intro-to-cross-validation-with-the-boston-data\"></a>\n",
        "### Intro to Cross-Validation With the Boston Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYl1c3IaZBDl"
      },
      "source": [
        "#### Create a cross-valiation with five folds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-dUGaT74ZBDl"
      },
      "outputs": [],
      "source": [
        "from sklearn import model_selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "J5GvJEgfZBDl"
      },
      "outputs": [],
      "source": [
        "kf = model_selection.KFold(n_splits=5, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston = load_boston()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtagb-F2aHml",
        "outputId": "d0e03fb3-d1a9-4ae6-feb0-d1be025b0989"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for both parts of data; don't forget to assign column names.\n",
        "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "y = pd.DataFrame(boston.target, columns=['MEDV'])"
      ],
      "metadata": {
        "id": "sVoVmF9eaLgi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Concatenate y and X, then overwrite the Boston variable."
      ],
      "metadata": {
        "id": "pKsRl6nraQCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "boston = pd.concat([y, X], axis=1)"
      ],
      "metadata": {
        "id": "fD8ZhTl-aO5t"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the class.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "xEkpRGZDaajP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4QCuxpfWZBDm",
        "outputId": "77d57373-9d78-4d06-9291-333b6b54b5fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~~~~ CROSS VALIDATION each fold ~~~~\n",
            "Model 1\n",
            "MSE: 22.670260626847824\n",
            "R2: 0.7392316073560956\n",
            "\n",
            "Model 2\n",
            "MSE: 20.87549134854232\n",
            "R2: 0.7382228661122677\n",
            "\n",
            "Model 3\n",
            "MSE: 31.88274677921655\n",
            "R2: 0.7389132446314226\n",
            "\n",
            "Model 4\n",
            "MSE: 15.528190811640888\n",
            "R2: 0.7382162991706053\n",
            "\n",
            "Model 5\n",
            "MSE: 25.896214572006034\n",
            "R2: 0.7388057391050495\n",
            "\n",
            "~~~~ SUMMARY OF CROSS VALIDATION ~~~~\n",
            "Mean of MSE for all folds: 23.37058082765072\n",
            "Mean of R2 for all folds: 0.7386779512750881\n"
          ]
        }
      ],
      "source": [
        "mse_values = []\n",
        "scores = []\n",
        "n = 0\n",
        "\n",
        "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    lr = LinearRegression().fit(X.iloc[train_index], y.iloc[train_index])\n",
        "    \n",
        "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lr.predict(X.iloc[test_index])))\n",
        "    scores.append(lr.score(X, y))\n",
        "    \n",
        "    n += 1\n",
        "    \n",
        "    print('Model {}'.format(n))\n",
        "    print('MSE: {}'.format(mse_values[n-1]))\n",
        "    print('R2: {}\\n'.format(scores[n-1]))\n",
        "\n",
        "\n",
        "print(\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
        "print('Mean of MSE for all folds: {}'.format(np.mean(mse_values)))\n",
        "print('Mean of R2 for all folds: {}'.format(np.mean(scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "i1UNQYn0ZBDm",
        "outputId": "cd9992a9-5c8a-45ee-e862-0f2671910177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23.690773340084284\n",
            "0.7048328908291578\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Note the results will vary each run since we take a different\n",
        "#   subset of the data each time (since shuffle=True)\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "print(np.mean(-cross_val_score(lr, X, y, cv=kf, scoring='neg_mean_squared_error')))\n",
        "print(np.mean(cross_val_score(lr, X, y, cv=kf)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90em0dj5ZBDm"
      },
      "source": [
        "<a id=\"three-way-data-split\"></a>\n",
        "## Three-Way Data Split\n",
        "---\n",
        "\n",
        "The most common workflow is actually a combination of train/test split and cross-validation. We take a train/test split on our data right away and try not spend a lot of time using the testing data set. Instead, we take our training data and tune our models using cross-validation. When we think we are done, we do one last test on the testing data to make sure we haven't accidently overfit to our training data.\n",
        "\n",
        "**If you tune hyperparameters via cross-validation, you should never use cross-validation on the same dataset to estimate OOS accuracy!** Using cross-validation in this way, the entire dataset was used to tune hyperparameters. So, this invalidates our condition above -- where we assumed the test set is a pretend \"out-of-sample\" dataset that was not used to train our model! So, we would expect the accuracy on this test set to be artificially inflated as compared to actual \"out-of-sample\" data.\n",
        "\n",
        "Even with good evaluation procedures, it is incredible easy to overfit our models by including features that will not be available during production or leak information about our testing data in other ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10QZIsNCZBDm"
      },
      "source": [
        "![](./assets/Train-Test-Split-CV.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "cyA4HA19ZBDm"
      },
      "source": [
        "- If model selection and true error estimates are to be computed simultaneously, three disjointed data sets are best.\n",
        "    - **Training set**: A set of examples used for learning – what parameters of the classifier?\n",
        "    - **Validation set**: A set of examples used to tune the parameters of the classifier.\n",
        "    - **Testing set**: A set of examples used ONLY to assess the performance of the fully trained classifier.\n",
        "- Validation and testing must be separate data sets. Once you have the final model set, you cannot do any additional tuning after testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "aEZ0HXB7ZBDn"
      },
      "source": [
        "1. Divide data into training, validation, and testing sets.\n",
        "2. Select architecture (model type) and training parameters (k).\n",
        "3. Train the model using the training set.\n",
        "4. Evaluate the model using the training set.\n",
        "5. Repeat 2–4 times, selecting different architectures (models) and tuning parameters.\n",
        "6. Select the best model.\n",
        "7. Assess the model with the final testing set."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "k_folds_cross_validation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
